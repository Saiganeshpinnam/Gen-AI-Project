# ğŸ¤– Enterprise Knowledge Assistant (RAG-based)

An end-to-end **Retrieval-Augmented Generation (RAG)** application that allows users to ask questions from internal enterprise documents and receive **accurate, contextual, and confidence-aware answers** with optional **audio playback**.

This project demonstrates a **production-style GenAI architecture** using modern AI tooling and cloud deployment.

---

## ğŸš€ Live Demo

Deployed on **Streamlit Cloud**  
ğŸ‘‰ (Your deployed Streamlit URL goes here)

---

## ğŸ“Œ Problem Statement

Enterprise knowledge is scattered across:
- PDFs
- HR policy documents
- Internal manuals and wikis

Employees spend significant time searching for answers or repeatedly asking support teams.

---

## ğŸ’¡ Solution

This application uses **Retrieval-Augmented Generation (RAG)** to:
- Semantically search enterprise documents
- Retrieve only relevant content
- Generate grounded LLM responses
- Detect low-confidence answers
- Provide audio summaries for hands-free usage

---

## ğŸ—ï¸ Architecture Overview

User Query
â†“
HuggingFace Embeddings
â†“
Pinecone Vector Database
â†“
Relevant Context Retrieval
â†“
Groq LLM (Llama 3)
â†“
Agent Confidence Check
â†“
Text + Audio Output (Streamlit UI)


---

## ğŸ”‘ Key Features

- âœ… Multi-format document ingestion (PDF, text)
- âœ… Semantic vector search
- âœ… Retrieval-Augmented Generation (RAG)
- âœ… Confidence-based agent logic
- âœ… Audio output (Text-to-Speech)
- âœ… Streamlit chat-style UI
- âœ… Cloud deployment with auto-redeploy

---

## ğŸ› ï¸ Tech Stack

### Frontend
- Streamlit

### AI & Backend
- LangChain
- HuggingFace Embeddings
- Groq (Llama 3.1)
- Pinecone Vector Database

### Text-to-Speech
- Google Text-to-Speech (gTTS)

### Deployment
- Streamlit Cloud
- GitHub

---

## ğŸ“‚ Project Structure

rag-project/
â”‚
â”œâ”€â”€ backend/
â”‚ â”œâ”€â”€ app.py # Streamlit UI
â”‚ â”œâ”€â”€ rag.py # RAG + Agent logic
â”‚ â”œâ”€â”€ ingestion.py # Text ingestion
â”‚ â”œâ”€â”€ pdf_ingestion.py # PDF ingestion
â”‚ â”œâ”€â”€ tts.py # Text-to-Speech
â”‚ â”œâ”€â”€ requirements.txt
â”‚ â”œâ”€â”€ Procfile
â”‚ â”œâ”€â”€ runtime.txt
â”‚
â”œâ”€â”€ .gitignore
â”œâ”€â”€ README.md


---

## âš™ï¸ Environment Variables

Set the following variables in **Streamlit Cloud â†’ App Settings â†’ Secrets**:

```env
PINECONE_API_KEY=your_pinecone_api_key
PINECONE_INDEX_NAME=rag-index
GROQ_API_KEY=your_groq_api_key

â— Do NOT commit .env files to GitHub.

ğŸ“¥ Document Ingestion

Run ingestion locally before deployment:

python ingestion.py
python pdf_ingestion.py


This will:

Chunk documents

Generate embeddings

Store vectors in Pinecone

ğŸ” Retrieval & Answer Generation (RAG)

User query â†’ embeddings

Retrieve top-k relevant chunks from Pinecone

Provide context to LLM

Generate grounded answer

Evaluate confidence score

ğŸ§  Agent Layer Logic
Condition	Action
High similarity score	Answer user
Low similarity score	Escalate / fallback
No context	Respond with insufficient information

ğŸ”Š Audio Output

Generated answers are converted to speech

Audio is playable directly in the UI

ğŸš€ Deployment (Streamlit Cloud)

Push code to GitHub

Create app on Streamlit Cloud

Set Main file path:

backend/app.py


Add environment secrets

App auto-redeploys on every GitHub push

ğŸ”Š Audio Output

Generated answers are converted to speech

Audio is playable directly in the UI

ğŸš€ Deployment (Streamlit Cloud)

Push code to GitHub

Create app on Streamlit Cloud

Set Main file path:

backend/app.py


Add environment secrets

App auto-redeploys on every GitHub push

ğŸ“ˆ Future Enhancements

Slack / Email escalation

User authentication

Analytics dashboard

Role-based document access

Multi-language support

Streaming responses

ğŸ§‘â€ğŸ’¼ Interview Highlights

Real-world enterprise RAG implementation

Clear separation of layers

Agent-based decision making

Cloud deployment

Scalable architecture








